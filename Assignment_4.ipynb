{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same first two preprocessing cells from part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import time\n",
    "import numpy as np\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # needed by word_tokenize\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# list of words found to still rank very high (10k+) after a running with just stopwords and examining vocab.txt\n",
    "other_words = ['lines', 'subject', 'would', 'organization']\n",
    "\n",
    "# Assuming this file is put under the same parent directoray as the data directory, and the data directory is named \"20news-train\"\n",
    "root_path = \"./20news-train\"\n",
    "# The maximum size of the final vocabulary. It's a hyper-parameter. You can change it to see what value gives the best performance.\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "start_time = time.time()\n",
    "vocab_full = {}\n",
    "n_doc = 0\n",
    "# Only keep the data dictionaries and ignore possible system files like .DS_Store\n",
    "folders = [os.path.join(root_path, name) for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        file = os.path.join(folder, filename)\n",
    "        n_doc += 1\n",
    "        # print(file)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                # split contractions into two words\n",
    "                line = contractions.fix(line)\n",
    "                tokens = word_tokenize(line)\n",
    "                # force everything to lower case and remove non-alphabetic characters\n",
    "                tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "                for token in tokens:\n",
    "                    # remove stop words, other words (above) and single characters\n",
    "                    if (token not in stop_words) and (token not in other_words) and (len(token) > 1):\n",
    "                        vocab_full[token] = vocab_full.get(token, 0) + 1\n",
    "print(f'{n_doc} documents in total with a total vocab size of {len(vocab_full)}')\n",
    "vocab_sorted = sorted(vocab_full.items(), key=operator.itemgetter(1), reverse=True)\n",
    "vocab_truncated = vocab_sorted[:MAX_VOCAB_SIZE]\n",
    "# Save the vocabulary to file for visual inspection and possible analysis\n",
    "with open('vocab1.txt', 'w') as f:\n",
    "    for vocab, freq in vocab_truncated:\n",
    "        f.write(f'{vocab}\\t{freq}\\n')\n",
    "# The final vocabulary is a dict mapping each token to its id. frequency information is not needed anymore.\n",
    "vocab = dict([(token, id) for id, (token, _) in enumerate(vocab_truncated)])\n",
    "# Since we have truncated the vocabulary, we will encounter many tokens that are not in the vocabulary. We will map all of them to the same 'UNK' token (a common practice in text processing), so we append it to the end of the vocabulary.\n",
    "vocab['UNK'] = MAX_VOCAB_SIZE\n",
    "vocab_size = len(vocab)\n",
    "unk_id = MAX_VOCAB_SIZE\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Vocabulary construction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have truncated the vocabulary, it's now reasonable to hold the entire feature matrix in memory (it takes about 3.6GB on a 64-bit machine). If memory is an issue, you could make the vocabulary even smaller or use sparse matrix.\n",
    "start_time = time.time()\n",
    "features = np.zeros((n_doc, vocab_size), dtype=int)\n",
    "print(f'The feature matrix takes {sys.getsizeof(features)} Bytes.')\n",
    "# The class label of each document\n",
    "labels = np.zeros(n_doc, dtype=int)\n",
    "# The mapping from the name of each class label (i.e., the subdictionary name corresponding to a topic) to an integer ID\n",
    "label2id = {}\n",
    "label_id = 0\n",
    "doc_id = 0\n",
    "for folder in folders:\n",
    "    label2id[folder] = label_id\n",
    "    for filename in os.listdir(folder):\n",
    "        labels[doc_id] = label_id\n",
    "        file = os.path.join(folder, filename)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                tokens = word_tokenize(line)\n",
    "                for token in tokens:\n",
    "                    # if the current token is in the vocabulary, get its ID; otherwise, get the ID of the UNK token\n",
    "                    token_id = vocab.get(token, unk_id)\n",
    "                    features[doc_id, token_id] += 1\n",
    "        doc_id += 1\n",
    "    label_id += 1\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Feature extraction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Kmeans:\n",
    "    def __init__(self, num_clusters, feature_matrix):\n",
    "        self.k = num_clusters\n",
    "        # get the shape of the matrix [# of docs, # of features (max vocab size)]\n",
    "        dimensionality = feature_matrix.shape\n",
    "        # calculate the min and max value for each feature, to be used in centroid initialization (earlier runs showed\n",
    "        # min to always be 0)\n",
    "        ranges = np.zeros((dimensionality[1]-1, 2))\n",
    "        for dim in range(dimensionality[1]-1):\n",
    "            ranges[dim, 0] = 0\n",
    "            ranges[dim, 1] = np.max(feature_matrix[:, dim])\n",
    "        # create k centroids, matching # of features stored for each doc, ignorining the UNK token b/c of its effect\n",
    "        # on skewing the ranges and resulting centroids\n",
    "        self.centroids = np.zeros((self.k, dimensionality[1]-1))\n",
    "        for i in range(self.k):\n",
    "            for dim in range(dimensionality[1]-1):\n",
    "                # random, uniform initialization\n",
    "                self.centroids[i, dim] = np.random.uniform(ranges[dim, 0], ranges[dim, 1], 1)\n",
    "        # initialize class variables in the __init__ function\n",
    "        self.cluster = []\n",
    "\n",
    "    def convergeClusters(self, features):\n",
    "        dimensions = features.shape\n",
    "\n",
    "        # track episodes till converge and prevent too long of running times (hopefully never 10k)\n",
    "        episodes = 0\n",
    "        not_converged = True\n",
    "        while episodes < 10000 and not_converged:\n",
    "            episodes += 1\n",
    "\n",
    "            # calculate the distance from each feature to the corresponding feature in every centroid\n",
    "            distances = np.zeros((dimensions[0],self.k))\n",
    "            for f_index, f_val in enumerate(features):\n",
    "                for c_index, c_val in enumerate(self.centroids):\n",
    "                    distances[f_index, c_index] = euclidean(f_val[:-1], c_val)\n",
    "            # assign each point to the minimum distance (closest) cluster\n",
    "            self.cluster = np.argmin(distances, axis = 1)\n",
    "\n",
    "            # gather all the points in the cluster.  if no points, do nothing for now, else compute the mean\n",
    "            # of that feature for that cluster\n",
    "            updated_centroids = np.zeros_like(self.centroids)\n",
    "            for centroid in range(self.k):\n",
    "                temp = features[self.cluster == centroid]\n",
    "                if len(temp) != 0:\n",
    "                    for dim in range(dimensions[1]-1):\n",
    "                        updated_centroids[centroid, dim] = np.mean(temp[:,dim])\n",
    "\n",
    "            # checks if the distance between centroids is smaller than the system eps\n",
    "            # (The smallest representable positive number such that 1.0 + eps != 1.0.) 2.22e-16 on my local machine\n",
    "            if np.linalg.norm(updated_centroids - self.centroids) < np.finfo(float).eps:\n",
    "                print(\"converged in {} episodes\".format(episodes))\n",
    "                not_converged = False\n",
    "\n",
    "            self.centroids = updated_centroids\n",
    "        print(\"converged in {} episodes\".format(episodes))\n",
    "        \n",
    "        \n",
    "    def evaluate(self, test_features):\n",
    "        # essentially the cluster identifying logic of the converge function, just not in a loop and returns the\n",
    "        # labels.\n",
    "        dimensions = test_features.shape\n",
    "\n",
    "        distances = np.zeros((dimensions[0], self.k))\n",
    "        for f_index, f_val in enumerate(test_features):\n",
    "            for c_index, c_val in enumerate(self.centroids):\n",
    "                distances[f_index, c_index] = euclidean(f_val[:-1], c_val)\n",
    "        # assign each point to the minimum distance (closest) cluster\n",
    "        test_labels = np.argmin(distances, axis=1)\n",
    "        return test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converge the clusters on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = Kmeans(20, features)\n",
    "kmeans.convergeClusters(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-use first two cells, just changing the path root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import time\n",
    "import numpy as np\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # needed by word_tokenize\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# list of words found to still rank very high (10k+) after a running with just stopwords and examining vocab.txt\n",
    "other_words = ['lines', 'subject', 'would', 'organization']\n",
    "\n",
    "# Assuming this file is put under the same parent directoray as the data directory, and the data directory is named \"20news-train\"\n",
    "root_path = \"./20news-test\"\n",
    "# The maximum size of the final vocabulary. It's a hyper-parameter. You can change it to see what value gives the best performance.\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "start_time = time.time()\n",
    "vocab_full = {}\n",
    "n_doc = 0\n",
    "# Only keep the data dictionaries and ignore possible system files like .DS_Store\n",
    "folders = [os.path.join(root_path, name) for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        file = os.path.join(folder, filename)\n",
    "        n_doc += 1\n",
    "        # print(file)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                # split contractions into two words\n",
    "                line = contractions.fix(line)\n",
    "                tokens = word_tokenize(line)\n",
    "                # force everything to lower case and remove non-alphabetic characters\n",
    "                tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "                for token in tokens:\n",
    "                    # remove stop words, other words (above) and single characters\n",
    "                    if (token not in stop_words) and (token not in other_words) and (len(token) > 1):\n",
    "                        vocab_full[token] = vocab_full.get(token, 0) + 1\n",
    "print(f'{n_doc} documents in total with a total vocab size of {len(vocab_full)}')\n",
    "vocab_sorted = sorted(vocab_full.items(), key=operator.itemgetter(1), reverse=True)\n",
    "vocab_truncated = vocab_sorted[:MAX_VOCAB_SIZE]\n",
    "# Save the vocabulary to file for visual inspection and possible analysis\n",
    "with open('vocab1.txt', 'w') as f:\n",
    "    for vocab, freq in vocab_truncated:\n",
    "        f.write(f'{vocab}\\t{freq}\\n')\n",
    "# The final vocabulary is a dict mapping each token to its id. frequency information is not needed anymore.\n",
    "vocab = dict([(token, id) for id, (token, _) in enumerate(vocab_truncated)])\n",
    "# Since we have truncated the vocabulary, we will encounter many tokens that are not in the vocabulary. We will map all of them to the same 'UNK' token (a common practice in text processing), so we append it to the end of the vocabulary.\n",
    "vocab['UNK'] = MAX_VOCAB_SIZE\n",
    "vocab_size = len(vocab)\n",
    "unk_id = MAX_VOCAB_SIZE\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Vocabulary construction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have truncated the vocabulary, it's now reasonable to hold the entire feature matrix in memory (it takes about 3.6GB on a 64-bit machine). If memory is an issue, you could make the vocabulary even smaller or use sparse matrix.\n",
    "start_time = time.time()\n",
    "features = np.zeros((n_doc, vocab_size), dtype=int)\n",
    "print(f'The feature matrix takes {sys.getsizeof(features)} Bytes.')\n",
    "# The class label of each document\n",
    "labels = np.zeros(n_doc, dtype=int)\n",
    "# The mapping from the name of each class label (i.e., the subdictionary name corresponding to a topic) to an integer ID\n",
    "label2id = {}\n",
    "label_id = 0\n",
    "doc_id = 0\n",
    "for folder in folders:\n",
    "    label2id[folder] = label_id\n",
    "    for filename in os.listdir(folder):\n",
    "        labels[doc_id] = label_id\n",
    "        file = os.path.join(folder, filename)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                tokens = word_tokenize(line)\n",
    "                for token in tokens:\n",
    "                    # if the current token is in the vocabulary, get its ID; otherwise, get the ID of the UNK token\n",
    "                    token_id = vocab.get(token, unk_id)\n",
    "                    features[doc_id, token_id] += 1\n",
    "        doc_id += 1\n",
    "    label_id += 1\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Feature extraction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify the given test cell to set the classified labels to test_labels, then calculate accuracy as written.  (I believe that is what this cell is set up to do.  Takes the 'answers' from the documents in the second for loop, then calculates accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(test_labels))\n",
    "\n",
    "#save the map between the name of label to an interger ID\n",
    "label_names = os.listdir(\"./20news-train\")\n",
    "label_map = {}\n",
    "new_ID = 0\n",
    "for label in label_names:\n",
    "    label_map[label] = new_ID\n",
    "    new_ID +=1\n",
    "    \n",
    "#find the real label for the test dataset \n",
    "TEST_PATH = \"./20news-test\"\n",
    "test_folders = os.listdir(TEST_PATH)\n",
    "train_labels = []\n",
    "for folder in test_folders:\n",
    "    path = os.path.join(TEST_PATH,folder)\n",
    "    try:\n",
    "        files = os.listdir(path)\n",
    "    except NotADirectoryError:\n",
    "        continue\n",
    "    for file in files:\n",
    "        tmp = label_map[f]\n",
    "        train_labels.append(tmp)\n",
    "#print(len(train_labels))\n",
    "\n",
    "# use model trained a few cells ago along with the newly extracted features to classify the new documents\n",
    "test_labels = kmeans.evaluate(features)\n",
    "\n",
    "#calculate accuracy\n",
    "test_labels = np.array(test_labels).astype(int)\n",
    "train_labels = np.array(train_labels).astype(int)\n",
    "accuracy = np.sum(test_labels == train_labels)/test_labels.shape[0]\n",
    "print(\"Accuracy is \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
