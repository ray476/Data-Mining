{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Matthew\n",
      "[nltk_data]     Ray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# nltk version 3.4.5  installed with anaconda navigator, alternatively pip install --user -U nltk according to their website\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "\n",
    "# assumes the folder is present in the same folder as this notebook\n",
    "file_start = '.\\\\20news-train'\n",
    "# gets a list of the subfolders\n",
    "sub_folders = os.listdir(file_start)\n",
    "\n",
    "# d_matrix will be a dict of dicts.  the outer dict will have a key corresponding with the order the document was read\n",
    "# (which will match its place in the y_vector) and the value is the inner dict.  This inner dict will have keys of\n",
    "# words and values of their frequency\n",
    "d_matrix = {}\n",
    "y_vector = []\n",
    "total_files = 0\n",
    "\n",
    "\n",
    "# this function only adapted from\n",
    "# https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "# in addition to whitespace and special characters, nltk's stop_words were filtered\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def normalize_line(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document, assuming this is included in the allowed \"off the self tokenizers from libraries like nltk\"\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "# goes word by word, checking if it is already in the dictionary.  if yes, increase the frequency count\n",
    "# if no, add as a new key value pair\n",
    "def update_dictionary(dictionary, tokens):\n",
    "    for word in tokens:\n",
    "        if word in dictionary:\n",
    "            dictionary[word] += 1\n",
    "        else:\n",
    "            dictionary.update({word: 1})\n",
    "\n",
    "# opens the file at the given path.  goes line by line, normalizing the line, splitting on whitespace, then updating\n",
    "# the per file dictionary.  both the meta-data and the main body are processed\n",
    "def process_file(name, file_num):\n",
    "    with open(name) as f:\n",
    "        # dictionary to added to d_matrix\n",
    "        file_dict = {}\n",
    "        for line in f:\n",
    "            normed_line_list = normalize_line(line).split()\n",
    "            update_dictionary(file_dict, normed_line_list)\n",
    "        d_matrix.update({file_num: file_dict})\n",
    "\n",
    "# outer for runs the whole list of subfolders, inner for gets each filename in that folder.  creates a string with the whole\n",
    "# path from this directory to the file, then passes to process_file.  finally, updates the y_vector with the appropriate\n",
    "# class label as outlined in the hw2 doc\n",
    "for i in range(sub_folders.__len__()):\n",
    "    for filename in os.listdir(file_start + '\\\\' + sub_folders[i]):\n",
    "        full_path = file_start + '\\\\' + sub_folders[i] + '\\\\' + filename\n",
    "        process_file(full_path, total_files)\n",
    "        total_files += 1\n",
    "        # alt.atheism = 1 comp.graphics = 2 etc.\n",
    "        y_vector.append(i+1)\n",
    "\n",
    "# final thoughts/changes needed going forward: \n",
    "# was not able to get a full m*n matrix. (maybe one day ill stop procrastinating) Possibly keep a set of all previously \n",
    "# seen words, insert all these with frequency of 0 into the new file_dicts?  but then going back to the previous \n",
    "# dicts and and adding the new words they didn't have with a frequency of 0 seems like a waste of time and memory.  \n",
    "# Going through all the files once to get a full vocab and then again for frequencies likewise seems costly.  Being \n",
    "# able to do row and column sums in a full matrix/pandas dataframe could be nice but could also be accomplished with : \n",
    "# sum = 0\n",
    "# for i in range(d_matrix.__len__()):\n",
    "#   current_dict = d_matrix[i]\n",
    "#   if desired_word in current_dict:\n",
    "#       sum += current_dict[desired_word]\n",
    "# \n",
    "# Aside from this, more work needs to be done with cleaning the raw text.  currently email addresses have the @ and .\n",
    "# striped and then the whole address is joined together (e.g. ray.476@osu.edu becomes rayosuedu) which i suspect\n",
    "# will be useless going forward\n",
    "\n",
    "# these prints give an IOpub data rate exceeded (pycharm could handle it), the hw2 doc does not specifiy what exactly to output, \n",
    "# only \"how to interpret the output\"\n",
    "# print(d_matrix)\n",
    "# print(y_vector)\n",
    "\n",
    "# this was useful to anchor a breakpoint during debugging the explore the variables in the call stack at the end of \n",
    "# code execution, not sure if jupyter offers this\n",
    "print('done')\n",
    "\n",
    "# p.s. code developed in pycharm but veried to run in this notebook.  sorry if i didnt use markdown cells interlaced with code \n",
    "# cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
