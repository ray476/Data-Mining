{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Matthew\n",
      "[nltk_data]     Ray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Matthew\n",
      "[nltk_data]     Ray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './20news-train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a3a9dbbb394a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mn_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Only keep the data dictionaries and ignore possible system files like .DS_Store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mfolders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolders\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './20news-train'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import time\n",
    "import numpy as np\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # needed by word_tokenize\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# list of words found to still rank very high (10k+) after a running with just stopwords and examining vocab.txt\n",
    "other_words = ['lines', 'subject', 'would', 'organization']\n",
    "\n",
    "# Assuming this file is put under the same parent directoray as the data directory, and the data directory is named \"20news-train\"\n",
    "root_path = \"./20news-train\"\n",
    "# The maximum size of the final vocabulary. It's a hyper-parameter. You can change it to see what value gives the best performance.\n",
    "MAX_VOCAB_SIZE = 40000\n",
    "\n",
    "start_time = time.time()\n",
    "vocab_full = {}\n",
    "n_doc = 0\n",
    "# Only keep the data dictionaries and ignore possible system files like .DS_Store\n",
    "folders = [os.path.join(root_path, name) for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        file = os.path.join(folder, filename)\n",
    "        n_doc += 1\n",
    "        # print(file)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                # split contractions into two words\n",
    "                line = contractions.fix(line)\n",
    "                tokens = word_tokenize(line)\n",
    "                # force everything to lower case and remove non-alphabetic characters\n",
    "                tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "                for token in tokens:\n",
    "                    # remove stop words, other words (above) and single characters\n",
    "                    if (token not in stop_words) and (token not in other_words) and (len(token) > 1):\n",
    "                        vocab_full[token] = vocab_full.get(token, 0) + 1\n",
    "print(f'{n_doc} documents in total with a total vocab size of {len(vocab_full)}')\n",
    "vocab_sorted = sorted(vocab_full.items(), key=operator.itemgetter(1), reverse=True)\n",
    "vocab_truncated = vocab_sorted[:MAX_VOCAB_SIZE]\n",
    "# Save the vocabulary to file for visual inspection and possible analysis\n",
    "with open('vocab1.txt', 'w') as f:\n",
    "    for vocab, freq in vocab_truncated:\n",
    "        f.write(f'{vocab}\\t{freq}\\n')\n",
    "# The final vocabulary is a dict mapping each token to its id. frequency information is not needed anymore.\n",
    "vocab = dict([(token, id) for id, (token, _) in enumerate(vocab_truncated)])\n",
    "# Since we have truncated the vocabulary, we will encounter many tokens that are not in the vocabulary. We will map all of them to the same 'UNK' token (a common practice in text processing), so we append it to the end of the vocabulary.\n",
    "vocab['UNK'] = MAX_VOCAB_SIZE\n",
    "vocab_size = len(vocab)\n",
    "unk_id = MAX_VOCAB_SIZE\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Vocabulary construction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix takes 452605368 Bytes.\n",
      "Feature extraction took 55.472546339035034 seconds\n"
     ]
    }
   ],
   "source": [
    "# Since we have truncated the vocabulary, it's now reasonable to hold the entire feature matrix in memory (it takes about 3.6GB on a 64-bit machine). If memory is an issue, you could make the vocabulary even smaller or use sparse matrix.\n",
    "start_time = time.time()\n",
    "features = np.zeros((n_doc, vocab_size), dtype=int)\n",
    "print(f'The feature matrix takes {sys.getsizeof(features)} Bytes.')\n",
    "# The class label of each document\n",
    "labels = np.zeros(n_doc, dtype=int)\n",
    "# The mapping from the name of each class label (i.e., the subdictionary name corresponding to a topic) to an integer ID\n",
    "label2id = {}\n",
    "label_id = 0\n",
    "doc_id = 0\n",
    "for folder in folders:\n",
    "    label2id[folder] = label_id\n",
    "    for filename in os.listdir(folder):\n",
    "        labels[doc_id] = label_id\n",
    "        file = os.path.join(folder, filename)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                tokens = word_tokenize(line)\n",
    "                for token in tokens:\n",
    "                    # if the current token is in the vocabulary, get its ID; otherwise, get the ID of the UNK token\n",
    "                    token_id = vocab.get(token, unk_id)\n",
    "                    features[doc_id, token_id] += 1\n",
    "        doc_id += 1\n",
    "    label_id += 1\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Feature extraction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nearest neighbor functions\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "neighbors = 8\n",
    "training_rounds = 5\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "def get_neighbor_labels(train_x, train_y, test_row):\n",
    "\tdistances = list()\n",
    "\t# use i to mark the index of the train_row within train_x\n",
    "\ti = 0\n",
    "\tfor train_row in train_x:\n",
    "\t\tdist = euclidean(test_row, train_row)\n",
    "\t\tdistances.append((i, dist))\n",
    "\t\ti += 1\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbor_labels = list()\n",
    "\tfor i in range(neighbors):\n",
    "\t\t# get the index of the neighbors from the training data, then look up their label and add it\n",
    "\t\tlocation = distances[i][0]\n",
    "\t\tneighbor_labels.append(train_y[location])\n",
    "\treturn neighbor_labels\n",
    "\n",
    "\n",
    "def classify_one_row(train_x, train_y, test_row):\n",
    "\tnearest_labels = get_neighbor_labels(train_x, train_y, test_row)\n",
    "\t# ties are settled by selecting the smallest value\n",
    "\tmode_results = mode(nearest_labels)\n",
    "\t# if all values tie at 1, select the closest label\n",
    "\tif mode_results.count[0] == 1:\n",
    "\t\tresult = nearest_labels[0]\n",
    "\telse:\n",
    "\t\tresult = mode_results.mode[0]\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def classify_test_data(train_x, train_y, test_x):\n",
    "\tpredictions = list()\n",
    "\tfor test_row in test_x:\n",
    "\t\tpredictions.append(classify_one_row(train_x, train_y, test_row))\n",
    "\treturn predictions\n",
    "\n",
    "\n",
    "def accuracy(predicted_labels, test_y):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(predicted_labels)):\n",
    "\t\tif predicted_labels[i] == test_y[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\tscore = correct / float(len(test_y)) * 100\n",
    "\tprint('{} correct predictions were made for a score of {}%'.format(correct, score))\n",
    "\n",
    "\n",
    "def evaluate(features, labels):\n",
    "\tfor _ in range(training_rounds):\n",
    "\t\tx_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)\n",
    "\t\tpredictions = classify_test_data(x_train, y_train, x_test)\n",
    "\t\taccuracy(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run of knn, can probably skip\n",
    "evaluate(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the decision tree is a model that can be built, make a class so the tree can be on object built from node objects\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "# based on https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/tree/_classes.py#L585\n",
    "# and https://ysu1989.github.io/courses/sp20/cse5243/Classification-BasicConcepts.pdf\n",
    "class tree_classifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        self.n_classes_ = len(set(labels))  # labels are assumed to go from 0 to n-1\n",
    "        self.n_features_ = features.shape[1]\n",
    "        self.tree_ = self._grow_tree(features, labels)\n",
    "\n",
    "    def predict(self, features):\n",
    "        return [self._predict(inputs) for inputs in features]\n",
    "\n",
    "    def _gini(self, labels):\n",
    "        return 1.0 - sum((np.sum(labels == labels.size) / labels.size) ** 2 for c in range(self.n_classes_))\n",
    "\n",
    "    def _best_split(self, features, labels):\n",
    "        if labels.size <= 1:\n",
    "            return None, None\n",
    "\n",
    "        # Count of each class in the current node.\n",
    "        num_parent = [np.sum(labels == c) for c in range(self.n_classes_)]\n",
    "\n",
    "        # Gini of current node with no split\n",
    "        best_gini = 1.0 - sum((n / labels.size) ** 2 for n in num_parent)\n",
    "        best_index, best_threshold = None, None\n",
    "\n",
    "        # Loop through all features to determine best split\n",
    "        for index in range(self.n_features_):\n",
    "            # Sort data along selected feature.  threshold contains the count for the given feature(word).\n",
    "            # Labels is the label of the document\n",
    "            thresholds, labels = zip(*sorted(zip(features[:, index], labels)))\n",
    "\n",
    "            num_left = [0] * self.n_classes_\n",
    "            num_right = num_parent.copy()\n",
    "            for i in range(1, labels.size):  # try sliding threshold (27-34)\n",
    "                c = labels[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (labels.size - i)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "\n",
    "                # weighted average as on slide 21\n",
    "                gini = (i * gini_left + (labels.size - i) * gini_right) / labels.size\n",
    "\n",
    "                # The following condition is to make sure we don't try to split two\n",
    "                # points with identical values for that feature, as it is impossible\n",
    "                # (both have to end up on the same side of a split).\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_index = index\n",
    "                    best_threshold = (thresholds[i] + thresholds[i - 1]) / 2  # midpoint\n",
    "\n",
    "        return best_index, best_threshold\n",
    "\n",
    "    def _grow_tree(self, features, labels, depth=0):\n",
    "        # Population for each class in current node. The predicted class is the one with\n",
    "        # largest population.\n",
    "        num_samples_per_class = [np.sum(labels == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(\n",
    "            gini=self._gini(labels), num_samples=labels.size, num_samples_per_class=num_samples_per_class, predicted_class=predicted_class,)\n",
    "\n",
    "        # Split recursively until maximum depth is reached.\n",
    "        if depth < self.max_depth:\n",
    "            index, thr = self._best_split(features, labels)\n",
    "            if index is not None:\n",
    "                indices_left = features[:, index] < thr\n",
    "                X_left, y_left = features[indices_left], labels[indices_left]\n",
    "                X_right, y_right = features[~indices_left], labels[~indices_left]\n",
    "                node.feature_index = index\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree test, can also probably be skipped\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)\n",
    "tree = tree_classifier(5)\n",
    "tree.fit(x_train, y_train)\n",
    "predictions = list()\n",
    "for test_row in x_test:\n",
    "    predictions.append(tree.predict(test_row.reshape(1, -1)))\n",
    "    \n",
    "correct = 0\n",
    "i = 0\n",
    "for label in predictions:\n",
    "    if label[0] == y_test[i]:\n",
    "        correct += 1\n",
    "score = correct / float(len(y_test)) * 100\n",
    "print('{} correct predictions were made for a score of {}%'.format(correct, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Matthew\n",
      "[nltk_data]     Ray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Matthew\n",
      "[nltk_data]     Ray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 documents in total with a total vocab size of 72370\n",
      "Vocabulary test construction took 53.990609645843506 seconds\n"
     ]
    }
   ],
   "source": [
    "# preprocesses and feature selection of test set, copied from above with root path changed\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import time\n",
    "import numpy as np\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # needed by word_tokenize\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# list of words found to still rank very high (10k+) after a running with just stopwords and examining vocab.txt\n",
    "other_words = ['lines', 'subject', 'would', 'organization']\n",
    "\n",
    "# Assuming this file is put under the same parent directoray as the data directory, and the data directory is named \"20news-train\"\n",
    "root_path = \"./20news-test\"\n",
    "# The maximum size of the final vocabulary. It's a hyper-parameter. You can change it to see what value gives the best performance.\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "start_time = time.time()\n",
    "vocab_full_test = {}\n",
    "n_doc_test = 0\n",
    "# Only keep the data dictionaries and ignore possible system files like .DS_Store\n",
    "folders = [os.path.join(root_path, name) for name in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, name))]\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        file = os.path.join(folder, filename)\n",
    "        n_doc_test += 1\n",
    "        # print(file)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                # split contractions into two words\n",
    "                line = contractions.fix(line)\n",
    "                tokens = word_tokenize(line)\n",
    "                # force everything to lower case and remove non-alphabetic characters\n",
    "                tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "                for token in tokens:\n",
    "                    # remove stop words, other words (above) and single characters\n",
    "                    if (token not in stop_words) and (token not in other_words) and (len(token) > 1):\n",
    "                        vocab_full_test[token] = vocab_full_test.get(token, 0) + 1\n",
    "print(f'{n_doc_test} documents in total with a total vocab size of {len(vocab_full_test)}')\n",
    "vocab_sorted_test = sorted(vocab_full_test.items(), key=operator.itemgetter(1), reverse=True)\n",
    "vocab_truncated_test = vocab_sorted_test[:MAX_VOCAB_SIZE]\n",
    "# Save the vocabulary to file for visual inspection and possible analysis\n",
    "with open('vocab1.txt', 'w') as f:\n",
    "    for vocab_test, freq in vocab_truncated_test:\n",
    "        f.write(f'{vocab_test}\\t{freq}\\n')\n",
    "# The final vocabulary is a dict mapping each token to its id. frequency information is not needed anymore.\n",
    "vocab_test = dict([(token, id) for id, (token, _) in enumerate(vocab_truncated_test)])\n",
    "# Since we have truncated the vocabulary, we will encounter many tokens that are not in the vocabulary. We will map all of them to the same 'UNK' token (a common practice in text processing), so we append it to the end of the vocabulary.\n",
    "vocab_test['UNK'] = MAX_VOCAB_SIZE\n",
    "vocab_size_test = len(vocab_test)\n",
    "unk_id = MAX_VOCAB_SIZE\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Vocabulary test construction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_doc_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b1470c55ada3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Since we have truncated the vocabulary, it's now reasonable to hold the entire feature matrix in memory (it takes about 3.6GB on a 64-bit machine). If memory is an issue, you could make the vocabulary even smaller or use sparse matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfeatures_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_doc_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The feature matrix takes {sys.getsizeof(features)} Bytes.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# The class label of each document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_doc_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Since we have truncated the vocabulary, it's now reasonable to hold the entire feature matrix in memory (it takes about 3.6GB on a 64-bit machine). If memory is an issue, you could make the vocabulary even smaller or use sparse matrix.\n",
    "start_time = time.time()\n",
    "features_test = np.zeros((n_doc_test, vocab_size_test), dtype=int)\n",
    "print(f'The feature matrix takes {sys.getsizeof(features)} Bytes.')\n",
    "# The class label of each document\n",
    "labels_test = np.zeros(n_doc_test, dtype=int)\n",
    "# The mapping from the name of each class label (i.e., the subdictionary name corresponding to a topic) to an integer ID\n",
    "label2id = {}\n",
    "label_id = 0\n",
    "doc_id = 0\n",
    "for folder in folders:\n",
    "    label2id[folder] = label_id\n",
    "    for filename in os.listdir(folder):\n",
    "        labels_test[doc_id] = label_id\n",
    "        file = os.path.join(folder, filename)\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                tokens = word_tokenize(line)\n",
    "                for token in tokens:\n",
    "                    # if the current token is in the vocabulary, get its ID; otherwise, get the ID of the UNK token\n",
    "                    token_id = vocab_test.get(token, unk_id)\n",
    "                    features_test[doc_id, token_id] += 1\n",
    "        doc_id += 1\n",
    "    label_id += 1\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Feature test extraction took {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time use the whole database, rather than the splitting done in evaluate.  the original features are labels are\n",
    "# like the train x/y and the test features and labels the test x/y\n",
    "test_predicitons = classify_test_data(features, labels, features_test)\n",
    "accuracy(test_predicitions, labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
